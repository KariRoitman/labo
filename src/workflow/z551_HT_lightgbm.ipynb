{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning  lightgbm\n",
    "\n",
    "#Necesita para correr en Google Cloud\n",
    "# 128 GB de memoria RAM\n",
    "#   8 vCPU\n",
    "\n",
    "# pensado para datasets con UNDERSAPLING de la clase mayoritaria\n",
    "\n",
    "#limpio la memoria\n",
    "rm( list= ls(all.names= TRUE) )  #remove all objects\n",
    "gc( full= TRUE )                 #garbage collection\n",
    "\n",
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"yaml\")\n",
    "\n",
    "require(\"lightgbm\")\n",
    "\n",
    "#paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "options(error = function() { \n",
    "  traceback(20); \n",
    "  options(error = NULL); \n",
    "  stop(\"exiting after script error\") \n",
    "})\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "#Parametros del script\n",
    "PARAM  <- list()\n",
    "PARAM$experimento <- \"HT5510\"\n",
    "\n",
    "PARAM$exp_input  <- \"TS5410\"\n",
    "\n",
    "PARAM$lgb_crossvalidation_folds  <- 5  #En caso que se haga cross validation, se usa esta cantidad de folds\n",
    "\n",
    "PARAM$lgb_semilla  <- 201107\n",
    "\n",
    "\n",
    "#Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list( \n",
    "   boosting= \"gbdt\",               #puede ir  dart  , ni pruebe random_forest\n",
    "   objective= \"binary\",\n",
    "   metric= \"custom\",\n",
    "   first_metric_only= TRUE,\n",
    "   boost_from_average= TRUE,\n",
    "   feature_pre_filter= FALSE,\n",
    "   force_row_wise= TRUE,           #para que los alumnos no se atemoricen con tantos warning\n",
    "   verbosity= -100,                # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "   min_sum_hessian_in_leaf= 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "   max_bin= 31L,                   #lo debo dejar fijo, no participa de la BO\n",
    "   num_iterations= 9999,           #un numero muy grande, lo limita early_stopping_rounds\n",
    "   pos_bagging_fraction= 1.0,      # 0.0 < pos_bagging_fraction <= 1.0\n",
    "   neg_bagging_fraction= 1.0,      # 0.0 < neg_bagging_fraction <= 1.0\n",
    "   is_unbalance=  FALSE,           # \n",
    "   scale_pos_weight= 1.0,          # scale_pos_weight > 0.0\n",
    "\n",
    "   drop_rate=  0.1,                # 0.0 < neg_bagging_fraction <= 1.0\n",
    "   max_drop= 50,                   # <=0 means no limit\n",
    "   skip_drop= 0.5,                 # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "   extra_trees= TRUE,             # Magic Sauce\n",
    "\n",
    "   seed=  PARAM$lgb_semilla\n",
    "   )\n",
    "\n",
    "\n",
    "#Aqui se cargan los hiperparametros que se optimizan en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet( \n",
    "         makeNumericParam(\"learning_rate\",    lower=    0.01, upper=     0.3),\n",
    "         makeNumericParam(\"feature_fraction\", lower=    0.01, upper=     1.0),\n",
    "         makeIntegerParam(\"num_leaves\",       lower=    500L,   upper=  2000L),\n",
    "         makeIntegerParam(\"min_data_in_leaf\", lower=    1L,   upper= 50000L),\n",
    "         makeIntegerParam(\"max_depth\", lower=  3L, upper= 12L,\n",
    "         makeIntegerParam(\"min_gain_to_split\", lower=0.05, upper= 0.15),\n",
    "         makeIntegerParam(\"bagging_fraction\", lower= 0.75, upper=0.90) \n",
    "         makeIntegerParam(\"lambda_l1\", lower= 0.0001, upper=1),\n",
    "         makeIntegerParam(\"lambda_l2\", lower= 0.0001, upper=1)                               \n",
    "        )\n",
    "\n",
    "\n",
    "#si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones  <- 60  #iteraciones de la Optimizacion Bayesiana\n",
    "\n",
    "# FIN Parametros del script\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#graba a un archivo los componentes de lista\n",
    "#para el primer registro, escribe antes los titulos\n",
    "\n",
    "exp_log  <- function( reg, arch=NA, folder=\"./exp/\", ext=\".txt\", verbose=TRUE )\n",
    "{\n",
    "  archivo  <- arch\n",
    "  if( is.na(arch) )  archivo  <- paste0(  folder, substitute( reg), ext )\n",
    "\n",
    "  if( !file.exists( archivo ) )  #Escribo los titulos\n",
    "  {\n",
    "    linea  <- paste0( \"fecha\\t\", \n",
    "                      paste( list.names(reg), collapse=\"\\t\" ), \"\\n\" )\n",
    "\n",
    "    cat( linea, file=archivo )\n",
    "  }\n",
    "\n",
    "  linea  <- paste0( format(Sys.time(), \"%Y%m%d %H%M%S\"),  \"\\t\",     #la fecha y hora\n",
    "                    gsub( \", \", \"\\t\", toString( reg ) ),  \"\\n\" )\n",
    "\n",
    "  cat( linea, file=archivo, append=TRUE )  #grabo al archivo\n",
    "\n",
    "  if( verbose )  cat( linea )   #imprimo por pantalla\n",
    "}\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol  <- 0\n",
    "GLOBAL_gan_max  <- -Inf\n",
    "vcant_optima  <- c()\n",
    "\n",
    "fganancia_lgbm_meseta  <- function( probs, datos) \n",
    "{\n",
    "  vlabels  <- get_field(datos, \"label\")\n",
    "  vpesos   <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol  <<- GLOBAL_arbol + 1\n",
    "  tbl  <- as.data.table( list( \"prob\"= probs, \n",
    "                               \"gan\" = ifelse( vlabels==1 & vpesos>1 , 117000, -3000 ) ) )\n",
    "\n",
    "  setorder( tbl, -prob )\n",
    "  tbl[ , posicion := .I ]\n",
    "  tbl[ , gan_acum :=  cumsum( gan ) ]\n",
    "\n",
    "  tbl[ , gan_suavizada := frollmean( x=gan_acum, n=2001, align=\"center\", na.rm=TRUE, hasNA= TRUE )  ]\n",
    "  gan  <- tbl[ , max(gan_suavizada, na.rm=TRUE) ]\n",
    "\n",
    "\n",
    "  pos  <- which.max( tbl[ , gan_suavizada ] ) \n",
    "  vcant_optima  <<- c( vcant_optima, pos )\n",
    "\n",
    "  if( GLOBAL_arbol %% 10 == 0 )\n",
    "  {\n",
    "    if( gan > GLOBAL_gan_max ) GLOBAL_gan_max  <<- gan \n",
    "\n",
    "    cat( \"\\r\" )\n",
    "    cat( \"Validate \", GLOBAL_iteracion, \" \" , \" \", GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \" )\n",
    "  }\n",
    "\n",
    "\n",
    "  return( list( \"name\"= \"ganancia\", \n",
    "                \"value\"=  gan,\n",
    "                \"higher_better\"= TRUE ) )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm  <- function( x )\n",
    "{\n",
    "  gc()\n",
    "  GLOBAL_iteracion  <<- GLOBAL_iteracion + 1\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo  <- c( PARAM$lgb_basicos,  x )\n",
    "\n",
    "  param_completo$early_stopping_rounds  <- as.integer(200 + 4/param_completo$learning_rate )\n",
    "\n",
    "  GLOBAL_arbol  <<- 0\n",
    "  GLOBAL_gan_max  <<- -Inf\n",
    "  vcant_optima  <<- c()\n",
    "  set.seed( PARAM$lgb_semilla )\n",
    "  modelo_train  <- lgb.train( data= dtrain,\n",
    "                              valids= list( valid= dvalidate ),\n",
    "                              eval=   fganancia_lgbm_meseta,\n",
    "                              param=  param_completo,\n",
    "                              verbose= -100 )\n",
    "\n",
    "  cat( \"\\n\" )\n",
    "\n",
    "  cant_corte  <- vcant_optima[ modelo_train$best_iter ]\n",
    "\n",
    "  #aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion  <- predict( modelo_train, \n",
    "                          data.matrix( dataset_test[ , campos_buenos, with=FALSE]) )\n",
    "\n",
    "  tbl  <- copy( dataset_test[ , list(\"gan\" = ifelse(clase_ternaria==\"BAJA+2\", 117000, -3000 )) ] )\n",
    "\n",
    "  tbl[ , prob := prediccion ]\n",
    "  setorder( tbl, -prob )\n",
    "  tbl[ , gan_acum :=  cumsum( gan ) ]\n",
    "  tbl[ , gan_suavizada :=  frollmean( x=gan_acum, n=2001, align=\"center\", na.rm=TRUE, hasNA= TRUE )  ]\n",
    "\n",
    "\n",
    "  ganancia_test  <- tbl[ , max(gan_suavizada, na.rm=TRUE) ]\n",
    "\n",
    "  cantidad_test_normalizada  <- which.max( tbl[ , gan_suavizada ] ) \n",
    "\n",
    "  rm( tbl )\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada  <- ganancia_test\n",
    "\n",
    "\n",
    "  #voy grabando las mejores column importance\n",
    "  if( ganancia_test_normalizada >  GLOBAL_ganancia )\n",
    "  {\n",
    "    GLOBAL_ganancia  <<- ganancia_test_normalizada\n",
    "    tb_importancia    <- as.data.table( lgb.importance( modelo_train ) )\n",
    "\n",
    "    fwrite( tb_importancia,\n",
    "            file= paste0( \"impo_\", GLOBAL_iteracion, \".txt\" ),\n",
    "            sep= \"\\t\" )\n",
    "\n",
    "    rm( tb_importancia )\n",
    "  }\n",
    "\n",
    "\n",
    "  #logueo final\n",
    "  ds  <- list( \"cols\"= ncol(dtrain),  \"rows\"= nrow(dtrain) )\n",
    "  xx  <- c( ds, copy(param_completo) )\n",
    "\n",
    "  xx$early_stopping_rounds  <- NULL\n",
    "  xx$num_iterations  <- modelo_train$best_iter\n",
    "  xx$estimulos  <- cantidad_test_normalizada\n",
    "  xx$ganancia  <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana  <- GLOBAL_iteracion\n",
    "\n",
    "  exp_log( xx,  arch= \"BO_log.txt\" )\n",
    "\n",
    "  return( ganancia_test_normalizada )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "#esta es la funcion mas mistica de toda la asignatura\n",
    "# sera explicada en  Laboratorio de Implementacion III\n",
    "\n",
    "vcant_optima   <- c()\n",
    "\n",
    "fganancia_lgbm_mesetaCV  <- function( probs, datos) \n",
    "{\n",
    "  vlabels  <- get_field(datos, \"label\")\n",
    "  vpesos   <- get_field(datos, \"weight\")\n",
    "\n",
    "  GLOBAL_arbol  <<- GLOBAL_arbol + 1\n",
    "\n",
    "  tbl  <- as.data.table( list( \"prob\"= probs, \n",
    "                               \"gan\" = ifelse( vlabels==1 & vpesos > 1,\n",
    "                                              117000,\n",
    "                                               -3000 ) ) )\n",
    "\n",
    "  setorder( tbl, -prob )\n",
    "  tbl[ , posicion := .I ]\n",
    "  tbl[ , gan_acum :=  cumsum( gan ) ]\n",
    "  tbl[  , gan_suavizada :=  frollmean( x=gan_acum, n=501, align=\"center\", na.rm=TRUE, hasNA= TRUE )  ]\n",
    "\n",
    "  gan  <-  tbl[ , max(gan_suavizada, na.rm=TRUE) ]\n",
    "\n",
    "  pos  <- which.max(  tbl[ , gan_suavizada ] ) \n",
    "\n",
    "  vcant_optima   <<- c( vcant_optima, pos )\n",
    "\n",
    "  if( GLOBAL_arbol %% (10*PARAM$lgb_crossvalidation_folds) == 0 )\n",
    "  {\n",
    "    if( gan > GLOBAL_gan_max ) GLOBAL_gan_max  <<- gan \n",
    "\n",
    "    cat( \"\\r\" )\n",
    "    cat( \"Cross Validate \", GLOBAL_iteracion, \" \" , \" \",\n",
    "         as.integer( GLOBAL_arbol/ PARAM$lgb_crossvalidation_folds), \"  \",\n",
    "         gan * PARAM$lgb_crossvalidation_folds, \"   \",\n",
    "         GLOBAL_gan_max * PARAM$lgb_crossvalidation_folds, \"   \" )\n",
    "  }\n",
    "\n",
    "  return( list( \"name\"= \"ganancia\", \n",
    "                \"value\"=  gan,\n",
    "                \"higher_better\"= TRUE ) )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbmCV  <- function( x )\n",
    "{\n",
    "  gc()\n",
    "  GLOBAL_iteracion  <<- GLOBAL_iteracion + 1\n",
    "\n",
    "  param_completo  <- c(PARAM$lgb_basicos,  x )\n",
    "\n",
    "  param_completo$early_stopping_rounds  <- as.integer(200 + 4/param_completo$learning_rate )\n",
    "\n",
    "  vcant_optima   <<- c()\n",
    "  GLOBAL_arbol  <<- 0\n",
    "  GLOBAL_gan_max  <<- -Inf\n",
    "  set.seed( PARAM$lgb_semilla )\n",
    "  modelocv  <- lgb.cv( data= dtrain,\n",
    "                       eval=   fganancia_lgbm_mesetaCV,\n",
    "                       param=  param_completo,\n",
    "                       stratified= TRUE,                   #sobre el cross validation\n",
    "                       nfold= PARAM$lgb_crossvalidation_folds,\n",
    "                       verbose= -100 )\n",
    "\n",
    "  cat(\"\\n\" )\n",
    "\n",
    "  desde  <- (modelocv$best_iter-1)*PARAM$lgb_crossvalidation_folds + 1\n",
    "  hasta  <- desde + PARAM$lgb_crossvalidation_folds -1\n",
    "\n",
    "  cant_corte   <-  as.integer( mean( vcant_optima[ desde:hasta ] ) * PARAM$lgb_crossvalidation_folds  )\n",
    "\n",
    "  ganancia  <- unlist(modelocv$record_evals$valid$ganancia$eval)[ modelocv$best_iter ]\n",
    "  ganancia_normalizada  <- ganancia * PARAM$lgb_crossvalidation_folds\n",
    "\n",
    "\n",
    "  if( ktest==TRUE )\n",
    "  {\n",
    "    #debo recrear el modelo\n",
    "    param_completo$early_stopping_rounds  <- NULL\n",
    "    param_completo$num_iterations  <- modelocv$best_iter\n",
    "\n",
    "    modelo  <- lgb.train( data= dtrain,\n",
    "                          param=  param_completo,\n",
    "                          verbose= -100 )\n",
    "\n",
    "    #aplico el modelo a testing y calculo la ganancia\n",
    "    prediccion  <- predict( modelo, \n",
    "                            data.matrix( dataset_test[ , campos_buenos, with=FALSE]) )\n",
    "\n",
    "    tbl  <- copy( dataset_test[ , list(\"gan\" = ifelse(clase_ternaria==\"BAJA+2\", 117000, -3000 )) ] )\n",
    "\n",
    "    tbl[ , prob := prediccion ]\n",
    "    setorder( tbl, -prob )\n",
    "    tbl[ , gan_acum :=  cumsum( gan ) ]\n",
    "    tbl[ , gan_suavizada :=  frollmean( x=gan_acum, n=2001, align=\"center\", na.rm=TRUE, hasNA= TRUE )  ]\n",
    "\n",
    "\n",
    "    #Dato que hay testing, estos valores son ahora los oficiales\n",
    "    ganancia_normalizada  <- tbl[ , max(gan_suavizada, na.rm=TRUE) ]\n",
    "    cant_corte  <- which.max( tbl[ , gan_suavizada ] ) \n",
    "\n",
    "    rm( tbl )\n",
    "    gc()\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "  #voy grabando las mejores column importance\n",
    "  if( ganancia_normalizada >  GLOBAL_ganancia )\n",
    "  {\n",
    "    GLOBAL_ganancia  <<- ganancia_normalizada\n",
    "\n",
    "    param_impo <-  copy( param_completo )\n",
    "    param_impo$early_stopping_rounds  <- 0\n",
    "    param_impo$num_iterations  <- modelocv$best_iter\n",
    "\n",
    "    modelo  <- lgb.train( data= dtrain,\n",
    "                       param=  param_impo,\n",
    "                       verbose= -100 )\n",
    "\n",
    "    tb_importancia  <- as.data.table( lgb.importance( modelo ) )\n",
    "\n",
    "    fwrite( tb_importancia,\n",
    "            file= paste0( \"impo_\", GLOBAL_iteracion, \".txt\" ),\n",
    "            sep= \"\\t\" )\n",
    "    \n",
    "    rm( tb_importancia )\n",
    "  }\n",
    "\n",
    "\n",
    "  #logueo final\n",
    "  ds  <- list( \"cols\"= ncol(dtrain),  \"rows\"= nrow(dtrain) )\n",
    "  xx  <- c( ds, copy(param_completo) )\n",
    "\n",
    "  xx$early_stopping_rounds  <- NULL\n",
    "  xx$num_iterations  <- modelocv$best_iter\n",
    "  xx$estimulos   <- cant_corte\n",
    "  xx$ganancia  <- ganancia_normalizada\n",
    "  xx$iteracion_bayesiana  <- GLOBAL_iteracion\n",
    "\n",
    "  exp_log( xx,  arch= \"BO_log.txt\" )\n",
    "\n",
    "  return( ganancia_normalizada )\n",
    "}\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------\n",
    "#Aqui empieza el programa\n",
    "PARAM$stat$time_start  <- format(Sys.time(), \"%Y%m%d %H%M%S\")\n",
    "\n",
    "setwd(\"~/buckets/b1/\")\n",
    "\n",
    "#cargo el dataset donde voy a entrenar\n",
    "#esta en la carpeta del exp_input y siempre se llama  dataset_training.csv.gz\n",
    "dataset_input  <- paste0( \"./exp/\", PARAM$exp_input, \"/dataset_training.csv.gz\" )\n",
    "dataset  <- fread( dataset_input )\n",
    "\n",
    "dataset[ , azar :=  NULL ]\n",
    "\n",
    "#Verificaciones\n",
    "if( ! (\"fold_train\"    %in% colnames(dataset) ) ) stop(\"Error, el dataset no tiene el campo fold_train \\n\")\n",
    "if( ! (\"fold_validate\" %in% colnames(dataset) ) ) stop(\"Error, el dataset no tiene el campo fold_validate \\n\")\n",
    "if( ! (\"fold_test\"     %in% colnames(dataset) ) ) stop(\"Error, el dataset no tiene el campo fold_test  \\n\")\n",
    "if( dataset[ fold_train==1, .N ] == 0 ) stop(\"Error, en el dataset no hay registros con fold_train==1 \\n\")\n",
    "\n",
    "#creo la carpeta donde va el experimento\n",
    "dir.create( paste0( \"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE )\n",
    "setwd(paste0( \"./exp/\", PARAM$experimento, \"/\"))   #Establezco el Working Directory DEL EXPERIMENTO\n",
    "\n",
    "write_yaml( PARAM, file= \"parametros.yml\" )   #escribo parametros utilizados\n",
    "\n",
    "cat( PARAM$exp_input,\n",
    "     file= \"TrainingStrategy.txt\",\n",
    "     append= FALSE )\n",
    "\n",
    "#defino la clase binaria clase01\n",
    "dataset[  , clase01 := ifelse( clase_ternaria==\"CONTINUA\", 0L, 1L ) ]\n",
    "\n",
    "\n",
    "#los campos que se pueden utilizar para la prediccion\n",
    "campos_buenos  <- setdiff( copy(colnames( dataset )), c( \"clase01\", \"clase_ternaria\", \"fold_train\", \"fold_validate\", \"fold_test\" ) )\n",
    "\n",
    "#la particion de train siempre va\n",
    "dtrain  <- lgb.Dataset( data=    data.matrix( dataset[ fold_train==1, campos_buenos, with=FALSE] ),\n",
    "                        label=   dataset[ fold_train==1, clase01 ],\n",
    "                        weight=  dataset[ fold_train==1, ifelse( clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "                                                                 ifelse( clase_ternaria == \"BAJA+1\", 1.0, 1.0) )],\n",
    "                        free_raw_data= FALSE\n",
    "                      )\n",
    "\n",
    "\n",
    "kvalidate  <- FALSE\n",
    "ktest  <- FALSE\n",
    "kcrossvalidation  <- TRUE\n",
    "\n",
    "#Si hay que hacer validacion\n",
    "if( dataset[ fold_train==0 & fold_test==0 & fold_validate==1, .N ] > 0 )\n",
    "{\n",
    "  kcrossvalidation  <- FALSE\n",
    "  kvalidate  <- TRUE\n",
    "  dvalidate  <- lgb.Dataset( data=  data.matrix( dataset[ fold_validate==1, campos_buenos, with=FALSE] ),\n",
    "                             label= dataset[ fold_validate==1, clase01 ],\n",
    "                             weight= dataset[ fold_validate==1, ifelse( clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "                                                                     ifelse( clase_ternaria == \"BAJA+1\", 1.0, 1.0) )],\n",
    "                             free_raw_data= FALSE  )\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#Si hay que hacer testing\n",
    "if( dataset[ fold_train==0 & fold_validate==0 & fold_test==1, .N ] > 0 )\n",
    "{\n",
    "  ktest  <- TRUE\n",
    "  campos_buenos_test  <- setdiff( copy(colnames( dataset )), c( \"fold_train\", \"fold_validate\", \"fold_test\" ) )\n",
    "  dataset_test  <- dataset[ fold_test== 1, campos_buenos_test, with=FALSE ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "rm( dataset )\n",
    "gc()\n",
    "\n",
    "\n",
    "#si ya existe el archivo log, traigo hasta donde procese\n",
    "if( file.exists( \"BO_log.txt\" ) )\n",
    "{\n",
    "  tabla_log  <- fread( \"BO_log.txt\" )\n",
    "  GLOBAL_iteracion  <- nrow( tabla_log )\n",
    "  GLOBAL_ganancia   <- tabla_log[ , max(ganancia) ]\n",
    "  rm(tabla_log)\n",
    "} else  {\n",
    "  GLOBAL_iteracion  <- 0\n",
    "  GLOBAL_ganancia   <- -Inf\n",
    "}\n",
    "\n",
    "\n",
    "#Aqui comienza la configuracion de mlrMBO\n",
    "\n",
    "#deobo hacer cross validation o  Train/Validate/Test\n",
    "if( kcrossvalidation ) {\n",
    "  funcion_optimizar  <- EstimarGanancia_lightgbmCV\n",
    "} else {\n",
    "  funcion_optimizar  <- EstimarGanancia_lightgbm\n",
    "}\n",
    "\n",
    "\n",
    "configureMlr( show.learner.output= FALSE)\n",
    "\n",
    "#configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "#por favor, no desesperarse por lo complejo\n",
    "obj.fun  <- makeSingleObjectiveFunction(\n",
    "              fn=       funcion_optimizar, #la funcion que voy a maximizar\n",
    "              minimize= FALSE,   #estoy Maximizando la ganancia\n",
    "              noisy=    TRUE,\n",
    "              par.set=  PARAM$bo_lgb,     #definido al comienzo del programa\n",
    "              has.simple.signature = FALSE   #paso los parametros en una lista\n",
    "             )\n",
    "\n",
    "#archivo donde se graba y cada cuantos segundos\n",
    "ctrl  <- makeMBOControl( save.on.disk.at.time= 600,  \n",
    "                         save.file.path=       \"bayesiana.RDATA\" )\n",
    "\n",
    "ctrl  <- setMBOControlTermination( ctrl, \n",
    "                                   iters= PARAM$bo_iteraciones )   #cantidad de iteraciones\n",
    "\n",
    "ctrl  <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI() )\n",
    "\n",
    "#establezco la funcion que busca el maximo\n",
    "surr.km  <- makeLearner(\"regr.km\",\n",
    "                        predict.type= \"se\",\n",
    "                        covtype= \"matern3_2\",\n",
    "                        control= list(trace= TRUE) )\n",
    "\n",
    "\n",
    "\n",
    "#Aqui inicio la optimizacion bayesiana\n",
    "if( !file.exists( \"bayesiana.RDATA\" ) ) {\n",
    "\n",
    "  run  <- mbo(obj.fun, learner= surr.km, control= ctrl)\n",
    "\n",
    "} else {\n",
    "  #si ya existe el archivo RDATA, debo continuar desde el punto hasta donde llegue\n",
    "  #  usado para cuando se corta la virtual machine\n",
    "  run  <- mboContinue( \"bayesiana.RDATA\" )   #retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "PARAM$stat$time_end  <- format(Sys.time(), \"%Y%m%d %H%M%S\")\n",
    "write_yaml( PARAM, file= \"parametros.yml\" )   #escribo parametros utilizados\n",
    "\n",
    "#dejo la marca final\n",
    "cat( format(Sys.time(), \"%Y%m%d %H%M%S\"),\"\\n\",\n",
    "     file= \"zRend.txt\",\n",
    "     append= TRUE  )\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#suicidio,  elimina la maquina virtual directamente\n",
    "# para no tener que esperar a que termine una Bayesian Optimization \n",
    "# sino Google me sigue facturando a pesar de no estar procesando nada\n",
    "# Give them nothing, but take from them everything.\n",
    "\n",
    "system( \"sleep 10  && \n",
    "        export NAME=$(curl -X GET http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google') &&\n",
    "        export ZONE=$(curl -X GET http://metadata.google.internal/computeMetadata/v1/instance/zone -H 'Metadata-Flavor: Google') &&\n",
    "        gcloud --quiet compute instances delete $NAME --zone=$ZONE\",\n",
    "        wait=FALSE )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
